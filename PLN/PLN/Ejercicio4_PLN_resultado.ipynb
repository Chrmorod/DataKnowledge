{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9dIPyXC-k1d"
      },
      "source": [
        "# INFO: Regular Expressions (python 3.x)\n",
        "\n",
        "https://docs.python.org/3/library/re.html\n",
        "\n",
        "https://docs.python.org/3/howto/regex.html#regex-howto\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFHx3Nes-k1j"
      },
      "source": [
        "# EJERCICIO 1: Quitar los signos de puntuaci√≥n de la siguiente cadena:  ??. ppi. ¬øcasa?.. COSA. ??perro. ¬øquesito? \"q√ºesti√≥\" anar-hi.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "On1dPAjk-k1k",
        "outputId": "bd72693c-ab6f-4ebe-f991-b8f77d010c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "match ...\n",
            "None\n",
            "search ...\n",
            "<re.Match object; span=(6, 11), match='¬øppi.'>\n",
            "grupo 1: ¬ø\n",
            "grupo 2: ppi\n",
            "grupo 3: .\n",
            "¬øppi.\n",
            "->name:  ¬ø\n",
            "(6, 11)\n",
            "6\n",
            "11\n",
            "cadena= ¬øppi.\n",
            "-------------------------------------------------\n",
            "referencia a grupos a la izquierda ya analizados\n",
            "-------------------------------------------------\n",
            "<re.Match object; span=(0, 9), match='la la lam'>\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "frase=\"$ & % ¬øppi. PEPE ¬øcasa?.. COSA. ??perro. ¬øquesito? q√ºesti√≥ anar-hi.\"\n",
        "#Match: al principio de la cadena\n",
        "print(\"match ...\")\n",
        "x=re.match(r'(?P<principi>[?.,;¬ø]*)([\\w]+)([?.,;¬ø]*)', frase)\n",
        "print(x)\n",
        "if x:\n",
        "    print (f\"grupo 1: {x.group(1)}\")\n",
        "    print (f\"grupo 2: {x.group(2)}\")\n",
        "    print (f\"grupo 3: {x.group(3)}\")\n",
        "if x:\n",
        "    print (x.group(0))\n",
        "\n",
        "#Search: la primera que encuentra en la cadena\n",
        "print(\"search ...\")\n",
        "x=re.search(r'(?P<principi>[?.,;¬ø]*)([\\w-]+)([?.,;¬ø]*)', frase)\n",
        "print(x)\n",
        "if x:\n",
        "    print (f\"grupo 1: {x.group(1)}\")\n",
        "    print (f\"grupo 2: {x.group(2)}\")\n",
        "    print (f\"grupo 3: {x.group(3)}\")\n",
        "if x:\n",
        "    print (x.group(0))\n",
        "\n",
        "print (\"->name: \",x.group('principi'))\n",
        "print (x.span())\n",
        "print (x.start())\n",
        "print (x.end())\n",
        "cadena=frase[x.start():x.end()]\n",
        "print (\"cadena=\",cadena)\n",
        "\n",
        "#referencia a grupos a la izquierda ya analizados\n",
        "print(\"-------------------------------------------------\")\n",
        "print(\"referencia a grupos a la izquierda ya analizados\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print (re.match(r'(la) \\1 (lam)',\"la la lam\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "H_Bh3oKv-k1n",
        "outputId": "be8fab6f-0f67-4f84-8ad3-7ef93ba966d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No encontrado ...\n"
          ]
        }
      ],
      "source": [
        "#Compilar la expresion\n",
        "pattern=re.compile (r'(?P<principi>[?.,;¬ø]*)([\\w-]+)([?.,;¬ø]*)',re.I|re.U)\n",
        "#pattern es la expresi√≥n regular compilada, y sobre ella se pueden utilizar los m√©todos match, search, findall, ...\n",
        "\n",
        "mat=pattern.match(frase)\n",
        "if mat:\n",
        "    print (mat.group(1))\n",
        "    print (mat.group(2))\n",
        "    print (mat.group(3))\n",
        "else:\n",
        "    print(\"No encontrado ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "tMUE4-3I-k1o",
        "outputId": "b4bf1812-dab8-4144-80fd-6bcb832e6180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬ø\n",
            "ppi\n",
            ".\n",
            "---------\n",
            "¬øppi.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sear=pattern.search(frase)\n",
        "if sear:\n",
        "    print (sear.group(1))\n",
        "    print (sear.group(2))\n",
        "    print (sear.group(3))\n",
        "    print(\"---------\")\n",
        "    print(sear.group(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "FjVw5vEY-k1q",
        "outputId": "a15ae0f0-c2c0-4790-9d2f-d84fafeeaae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<re.Match object; span=(6, 11), match='¬øppi.'>, <re.Match object; span=(12, 16), match='PEPE'>, <re.Match object; span=(17, 25), match='¬øcasa?..'>, <re.Match object; span=(26, 31), match='COSA.'>, <re.Match object; span=(32, 40), match='??perro.'>, <re.Match object; span=(41, 50), match='¬øquesito?'>, <re.Match object; span=(51, 58), match='q√ºesti√≥'>, <re.Match object; span=(59, 67), match='anar-hi.'>]\n"
          ]
        }
      ],
      "source": [
        "#Finditer: Todas las ocurrencias de la cadena\n",
        "fiiter=pattern.finditer(frase)\n",
        "print ([x for x in fiiter])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "p1lxyhJN-k1r",
        "outputId": "b0231f6e-4532-47d0-ce26-0fa6ce8e81a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬ø\n",
            "ppi\n",
            ".\n",
            "\n",
            "PEPE\n",
            "\n",
            "¬ø\n",
            "casa\n",
            "?..\n",
            "\n",
            "COSA\n",
            ".\n",
            "??\n",
            "perro\n",
            ".\n",
            "¬ø\n",
            "quesito\n",
            "?\n",
            "\n",
            "q√ºesti√≥\n",
            "\n",
            "\n",
            "anar-hi\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "#Finditer:\n",
        "fiiter=pattern.finditer(frase)\n",
        "for i in fiiter:\n",
        "    print (i.group(1))\n",
        "    print (i.group(2))\n",
        "    print (i.group(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "1TksAPsm-k1s",
        "outputId": "f4425973-3982-43a8-8852-321b6a620088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('¬ø', 'ppi', '.'), ('', 'PEPE', ''), ('¬ø', 'casa', '?..'), ('', 'COSA', '.'), ('??', 'perro', '.'), ('¬ø', 'quesito', '?'), ('', 'q√ºesti√≥', ''), ('', 'anar-hi', '.')]\n",
            "ppi\n",
            "PEPE\n",
            "casa\n",
            "COSA\n",
            "perro\n",
            "quesito\n",
            "q√ºesti√≥\n",
            "anar-hi\n"
          ]
        }
      ],
      "source": [
        "# Findall: Todas las palabras de la frase\n",
        "fiall=pattern.findall(frase)\n",
        "print (fiall)\n",
        "for i in fiall:\n",
        "    print (i[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO8QddEz-k1t"
      },
      "source": [
        "# EJERCICIO 2: \"sustituye la palabra eso por  3 guiones, pero OJO con queso, o beso, o en ESO en may√∫sculas\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "qWceKqzP-k1v",
        "outputId": "06028a75-468c-4c30-9868-5ee5057edba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sustituye la palabra eso por  3 guiones, pero OJO con: queso, o beso; ESO en may√∫sculas s√≠.\n",
            "sustituye la palabra --- por  3 guiones, pero OJO con: queso, o beso; --- en may√∫sculas s√≠.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "frase1='sustituye la palabra eso por  3 guiones, pero OJO con: queso, o beso; ESO en may√∫sculas s√≠.'\n",
        "print (frase1)\n",
        "susti=re.compile (r'(\\beso\\b)',re.I|re.U|re.X)\n",
        "x=re.sub(susti,\"---\",frase1)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50X9yNcT-k1w"
      },
      "source": [
        "# EJERCICIO 3: encontrar fechas con formato dd/mm/aaaa, dd/mm. El separador tambi√©n puede ser  un gui√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "1928yGKs-k1x"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "ejemplo=\"el 12/03-1987 el 23/03 o el 21-04 no \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "x89WkL9e-k1x"
      },
      "outputs": [],
      "source": [
        "date=r\"(\\d{2}(/|-)\\d{2}((/|-)\\d{2,4})?)\"\n",
        "pattern=re.compile (date,re.I|re.U)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "SjeJpGJu-k1x",
        "outputId": "c8dfd125-df3c-4d2e-ef13-a51d3d839308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/03-1987\n",
            "23/03\n",
            "21-04\n"
          ]
        }
      ],
      "source": [
        "fiiter=pattern.finditer(ejemplo)\n",
        "for i in fiiter:\n",
        "    print (ejemplo[i.start():i.end()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "0BwScsM9-k1y",
        "outputId": "4e28d509-4a87-4932-cd44-dd4170071b88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(3, 13), match='12/03-1987'>\n",
            "-\n"
          ]
        }
      ],
      "source": [
        "s=pattern.search(ejemplo)\n",
        "print (s)\n",
        "print (s.group(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "akmiq91z-k1z",
        "outputId": "0d6550f7-9cc4-4915-dc6e-7ebfe9363196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "m=pattern.match(ejemplo)\n",
        "print (m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "fc_DI1hq-k1z"
      },
      "outputs": [],
      "source": [
        "# EJERCICIO 4: definir una RE que reconozca las instancias de \"Dani Alvez\" del texto del ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "cVlpO4wW-k10",
        "outputId": "44c04a4e-3b6d-4740-9e9b-7bff94b2bb4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CON GRUPOS:\n",
            "----------\n",
            "('#dani', ' ', 'alves')\n",
            "('#daniel', ' ', 'alves')\n",
            "('#daniel', '', 'alves99_k')\n",
            "('@daniel_kk', ' ', 'alves')\n",
            "('', ' ', '#alves')\n",
            "('', ' ', 'alves')\n",
            "----------\n",
            "SIN GRUPOS:\n",
            "----------\n",
            "#dani alves\n",
            "#daniel alves\n",
            "#danielalves99_k\n",
            "@daniel_kk alves\n",
            " #alves\n",
            " alves\n",
            "----------\n",
            "CON 1 GRUPO findall:\n",
            "----------\n",
            "<class 'list'>\n",
            "['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n",
            "('#dani alves', 'a', ' ', 'alves')\n",
            "('#daniel alves', 'a', ' ', 'alves')\n",
            "('#danielalves99_k', 'a', '', 'alves99_k')\n",
            "('@daniel_kk alves', 'a', ' ', 'alves')\n",
            "(' #alves', '', ' ', '#alves')\n",
            "(' alves', '', ' ', 'alves')\n",
            "----------\n",
            "CON 1 GRUPO finiter:\n",
            "----------\n",
            "#dani alves\n",
            "a\n",
            " \n",
            "alves\n",
            "#daniel alves\n",
            "a\n",
            " \n",
            "alves\n",
            "#danielalves99_k\n",
            "a\n",
            "None\n",
            "alves99_k\n",
            "@daniel_kk alves\n",
            "a\n",
            " \n",
            "alves\n",
            " #alves\n",
            "None\n",
            " \n",
            "#alves\n",
            " alves\n",
            "None\n",
            " \n",
            "alves\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "texto= \"#dani alves #daniel alves #danielalves99_k daniel @daniel_kk alves #alves alves\"\n",
        "pattern_con_grupos=re.compile(r'([#@]?dani\\S*)? (\\s)* ([#@]?alves\\S*)+',re.I|re.X)\n",
        "pattern_sin_grupos=re.compile(r'(?:[#@]?dani\\S*)? (?:\\s)* (?:[#@]?alves\\S*)+',re.I|re.X)\n",
        "pattern_con_grup=re.compile(r'((?:[#@]?d(a)ni\\S*)? (\\s)* ([#@]?alves\\S*)+)',re.I|re.X)\n",
        "#IMPORTANTE: poner la opciones:\n",
        "# re.I: para olvidarte de may√∫sculas y min√∫sculas\n",
        "# re.X: para olvidarte de blancos y comentarios dentro de las expresiones regulares\n",
        "#        si no se pone y dejas un blanco, es una parte mas de regex\n",
        "# Ojo con los parentesis: cada vez que pones una expresi√≥n entre parentesis es un grupo\n",
        "# y a veces no interesa\n",
        "# Si pones por ejemplo (xxx) es un grupo, si no quieres grupo (?:xxx)\n",
        "\n",
        "#Con grupos\n",
        "print(\"CON GRUPOS:\")\n",
        "print(\"----------\")\n",
        "results_grupos=pattern_con_grupos.findall(texto)\n",
        "for f in results_grupos:\n",
        "    print (f)\n",
        "print(\"----------\")\n",
        "\n",
        "#Sin grupos\n",
        "print(\"SIN GRUPOS:\")\n",
        "print(\"----------\")\n",
        "results_sin_grupos=pattern_sin_grupos.findall(texto)\n",
        "for f in results_sin_grupos:\n",
        "    print (f)\n",
        "print(\"----------\")\n",
        "#Con 1 grupo findall\n",
        "print(\"CON 1 GRUPO findall:\")\n",
        "print(\"----------\")\n",
        "results_grup=pattern_con_grup.findall(texto)\n",
        "print (type(results_grup))\n",
        "print (dir(results_grup))\n",
        "for f in results_grup:\n",
        "    print (f)\n",
        "print(\"----------\")\n",
        "\n",
        "##Con 1 grupo finditer\n",
        "print(\"CON 1 GRUPO finiter:\")\n",
        "print(\"----------\")\n",
        "fiiter=pattern_con_grup.finditer(texto)\n",
        "for i in fiiter:\n",
        "    print (i.group(1))\n",
        "    print (i.group(2))\n",
        "    print (i.group(3))\n",
        "    print (i.group(4))\n",
        "print(\"----------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06RRYzBh-k10"
      },
      "source": [
        "# Ejercicio 5: Elongated words and censured words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "jl9Sswq8-k11",
        "outputId": "2f3f2ff5-935a-40bb-96b1-b835f3d464a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['soooo', 'hiiiii', 'whyyyy', 'callllllllla']\n"
          ]
        }
      ],
      "source": [
        "cad= 'soooo hiiiii whyyyy done calla callllllllla'\n",
        "import re\n",
        "elongated = re.compile(r\"(.)\\1{2}\")\n",
        "print ([word for word in cad.split() if elongated.search(word)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ne3V9USB-k11",
        "outputId": "ab2e22dc-e7aa-4ec4-d6de-b63d5f9a2f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "so hi why done calla cala\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "frase1='soooo hiiiii whyyyy done calla callllllllla'\n",
        "norm=re.compile (r\"(.)\\1{2,}\",re.I|re.U|re.X)\n",
        "x=re.sub(norm,r\"\\1\",frase1)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "4PXx6GTe-k12",
        "outputId": "c153b072-219b-4833-f179-82c181f8e0a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p**a', 'c**o', 'm*****n']\n"
          ]
        }
      ],
      "source": [
        "fraseC=\"p**a c**o puto m*****n\"\n",
        "censurado=re.compile (r'(?:\\b\\w+[*]+\\w+\\b)')\n",
        "print([word for word in fraseC.split() if censurado.search(word)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "2vcjd4HO-k12",
        "outputId": "8872e5ec-6557-42ea-c82f-398eb75c508b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', '√Ål2√≥', 'p1', 'p2']\n",
            "['Hola', '√Ål2√≥', 'p1', 'p2']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text=\"Hola √Ål2√≥. &%$p1 p2@#.!\"\n",
        "prova=re.compile (r'(\\w+)')\n",
        "prova1=re.compile(r'([\\w]+)')\n",
        "results=prova1.findall(text)\n",
        "results1=prova.findall(text)\n",
        "print (results)\n",
        "print(results1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "FFKKIjVj-k13",
        "outputId": "bbab8e9e-5a1f-4d93-be9a-0ffa1a27a232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['üíï', 'üë≠', 'üëô', 'üòä']\n",
            "['üíï', 'üë≠', 'üëô', 'üòä']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text=\":;.,üíïüë≠üëôüòä hola ll @ #\"\n",
        "emoticons=re.compile(\"[^\\w\\s\\.:;,@]\")\n",
        "emoticons=re.compile(\"[\\U00010000-\\U0010ffff]\")\n",
        "results=emoticons.findall(text)\n",
        "results1=emoticons.findall(text)\n",
        "print(results)\n",
        "print(results1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizador para el espa√±ol"
      ],
      "metadata": {
        "id": "mPrq6przL1K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construir un tokenizador para el espanÃÉol, que, dado un fichero de texto de entrada (entrada_tokenizador.txt), separe en tokens, y los muestre en un fichero de salida en el formato que se muestra en (salida_tokenizador.txt). Por lo menos el tokenizador deberaÃÅ funcionar correctamente para el ejemplo."
      ],
      "metadata": {
        "id": "3Mwe4ooTMHJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El tokenizador debe cumplir las siguientes restricciones:\n",
        "\n",
        "1) Los siÃÅmbolos que hay que separar de cada palabra son:( ). ,‚Äò‚Äú?¬ø!¬° ...; :\n",
        "\n",
        "2) No se deben separar los nuÃÅmeros decimales, ejemplo: 44,45 45.60\n",
        "\n",
        "3) No se deben separar fechas 12/12/90, 12-03-98, ni horas, 9:30 h.\n",
        "\n",
        "4) Las fechas en formato 12 de febrero de 2018 hay que mantenerlas como un token\n",
        "\n",
        "5) No se deben separar direcciones web http://www.colorin.com ni correos electroÃÅnicos\n",
        "xx@cdit.com\n",
        "\n",
        "6) Hay que mantener menciones a usuarios (@user) y hashtags (#hashtag) como se\n",
        "utilizan en Twitter.\n",
        "\n",
        "7) Hay que mantener acroÃÅnimos, por ejemplo: EE.UU., S.L., CC.OO., S.A., D., U.R.S.S, ...\n",
        "üë≠\n",
        "\n",
        "8) Respetar los emoticonos: üíï\n",
        "üëô\n",
        "üòä etc."
      ],
      "metadata": {
        "id": "6l4TsfbNMKSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ftLOr0MRB63",
        "outputId": "3cf70889-e897-4c40-c287-b60e3b021d4b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def tokenize(text):\n",
        "    # Expresiones regulares para identificar diferentes tipos de tokens:\n",
        "    # [\\U0001F300-\\U0001F6FF] Caras, animales, comida, vehiculos y banderas.\n",
        "    # [\\U0001F1E0-\\U0001F1FF] Banderas de diferentes paises.\n",
        "    # [\\U0001F900-\\U0001F9FF] Caracteres suplementarios relacionados con emoticonos, caras expresivas y gestos de manos.\n",
        "    # [\\U0001FA70-\\U0001FAFF] Caracteres relacionados con actividades deportivas.\n",
        "    emoticon_pattern = r'[\\U0001F300-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U0001F900-\\U0001F9FF]|[\\U0001FA70-\\U0001FAFF]'\n",
        "    # Patr√≥n para fecha entre 1-2 numeros por dia, podr√° ser en formato xx/xx/xxxx o xx-xx-xxxx\n",
        "    # 1-2 numeros por mes y entre 2-4 numeros para el a√±o.\n",
        "    #date_pattern = r'\\d{1,2}[\\/-]\\d{1,2}(?:[\\/-]\\d{2,4})?'\n",
        "    date_pattern = r'\\d{1,2}[\\/-]\\d{1,2}(?:[\\/-]\\d{2,4})?|\\d{1,2}\\s+de\\s+\\w+\\s+de\\s+\\d{4}'\n",
        "    # Patr√≥n para horas entre 1-2 numeros por horas y entre puntos : y luego los valores de dos 2 minutos\n",
        "    # y tambi√©n se tiene en cuenta que se representa despu√©s la aApP o la mM separada\n",
        "    # por punto. Interrogante ? es que puede ser opcional o puede estar o no puede estar.\n",
        "    #time_pattern = r'\\d{1,2}:\\d{2}?[aApP]\\.?[mM]\\.?'\n",
        "    time_pattern = r'\\b\\d{1,2}:\\d{2}(?: [ap]\\.?[mM]\\.?| h)?\\b'\n",
        "    # Patr√≥n para urls\n",
        "    url_pattern = r'https?://[\\w\\-\\.\\/]+[\\w\\-\\._~:/\\?#\\[\\]@!\\$&\\'\\(\\)\\*\\+,;=]+'\n",
        "    #url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    # Patr√≥n para email, la S representa uno o m√°s caracteres que no sean espacios en blanco.\n",
        "    email_pattern = r'\\S+@\\S+'\n",
        "    # Patr√≥n para los decimales donde se contempla punto o coma.\n",
        "    number_pattern = r'\\d+(?:[\\.,]\\d+)?'\n",
        "    # Patr√≥n para acronimos\n",
        "    acronym_pattern = r'\\b[A-Z\\.]+\\b\\.?'\n",
        "    # Patr√≥n del limite de palabra, palabra, limite de palabra\n",
        "    #word_pattern = r'\\b\\w+\\b'\n",
        "    word_pattern = r'\\b\\w+(?:-?\\w+)?\\b'\n",
        "    # Patr√≥n para los s√≠mbolos que se deben separar de las palabras\n",
        "    symbol_pattern = r'\\.{3}|[.,‚Äò‚Äú?¬ø!¬°;:%]'\n",
        "    # Patr√≥n para menciones de usario y hashtags\n",
        "    user_mention_pattern = r'@[\\w_]+'\n",
        "    #hashtag_pattern = r'#[\\w_]+'\n",
        "    hashtag_pattern = r'#[\\w_-]+'\n",
        "    # Tokenizador\n",
        "    tokens = []\n",
        "    for token in re.findall(f'{emoticon_pattern}|{date_pattern}|{time_pattern}|{url_pattern}|{email_pattern}|{number_pattern}|{acronym_pattern}|{word_pattern}|{symbol_pattern}|{user_mention_pattern}|{hashtag_pattern}', text):\n",
        "        tokens.append(token)\n",
        "    return tokens\n",
        "\n",
        "# Lectura del archivo de entrada\n",
        "with open('/content/drive/MyDrive/PLN-2022-2023/SAÃÅBADO/Ejercicios/entrada_tokenizador.txt', 'r', encoding='utf-8') as input_file:\n",
        "    text = input_file.read()\n",
        "\n",
        "# Tokenizaci√≥n del texto\n",
        "tokens = tokenize(text)\n",
        "\n",
        "# Escritura del archivo de salida\n",
        "with open('/content/drive/MyDrive/PLN-2022-2023/SAÃÅBADO/Ejercicios/salida_tokenizador_propio.txt', 'w', encoding='utf-8') as output_file:\n",
        "    output_file.write('\\n'.join(tokens))\n"
      ],
      "metadata": {
        "id": "nCypYRhZMPyJ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/PLN-2022-2023/SAÃÅBADO/Ejercicios/salida_tokenizador_propio.txt', 'r', encoding='utf-8') as input_file:\n",
        "    text_output = input_file.read()"
      ],
      "metadata": {
        "id": "X8waOy_FRjc2"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMqWPUXCR1s_",
        "outputId": "3ade84a4-67da-423d-df89-19189a48e8c2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√âl\n",
            ",\n",
            "Antonio\n",
            ",\n",
            "no\n",
            "vendr√°\n",
            "ma√±ana\n",
            ":\n",
            "lo\n",
            "har√°\n",
            "pasado\n",
            "ma√±ana\n",
            ".\n",
            "¬ø\n",
            "¬ø\n",
            "¬ø\n",
            "¬ø\n",
            "Cu√°ndo\n",
            "?\n",
            "?\n",
            "?\n",
            "?\n",
            "No\n",
            "te\n",
            "lo\n",
            "he\n",
            "dicho\n",
            "...\n",
            "¬°\n",
            "?\n",
            "Vale\n",
            "!\n",
            "no\n",
            "te\n",
            "he\n",
            "oido\n",
            ".\n",
            "De\n",
            "acuerdo\n",
            ";\n",
            "No\n",
            "ir√©\n",
            ".\n",
            "Pesa\n",
            "44.44\n",
            "kg\n",
            "y\n",
            "mide\n",
            "32,32\n",
            "m\n",
            ".\n",
            "El\n",
            "12-12-2020\n",
            ",\n",
            "y\n",
            "el\n",
            "13/12\n",
            ":\n",
            "habr√°\n",
            "examen\n",
            ",\n",
            "el\n",
            "14-12\n",
            "ya\n",
            "veremos\n",
            ".\n",
            "El\n",
            "pseudo-c√≥digo\n",
            "vale\n",
            "30,6\n",
            "sobre\n",
            "100\n",
            ".\n",
            "El\n",
            "15.5\n",
            "%\n",
            "no\n",
            "es\n",
            "suficiente\n",
            ".\n",
            "El\n",
            "bote\n",
            "est√°\n",
            "lleno\n",
            ",\n",
            "o\n",
            "vacio\n",
            "no\n",
            "semi-vacio\n",
            ".\n",
            "¬°\n",
            "Ay\n",
            "!\n",
            "el\n",
            "correo\n",
            "es\n",
            "fpla@dsic.upv.es\n",
            "y\n",
            "la\n",
            "web\n",
            ":\n",
            "http://users.dsic.upv.es/~fpla/\n",
            "se\n",
            "me\n",
            "olvidaba\n",
            ",\n",
            "ha\n",
            "cambiado\n",
            ".\n",
            "Ahora\n",
            "es\n",
            "http://personales.upv.es/~fpla/\n",
            "Ma√±ana\n",
            "nos\n",
            "vemos\n",
            "a\n",
            "las\n",
            "9:30\n",
            "horas\n",
            ".\n",
            "3/4\n",
            "partes\n",
            "de\n",
            "la\n",
            "poblaci√≥n\n",
            "come\n",
            "carne\n",
            ".\n",
            "el\n",
            "usuario\n",
            "@antonio_123\n",
            "escribi√≥\n",
            "un\n",
            "tweet\n",
            "con\n",
            "el\n",
            "hashtag\n",
            "#alc-2019\n",
            "el\n",
            "viernes\n",
            ",\n",
            "https://haha.ls-ps.com\n",
            "El\n",
            "14 de marzo de 2021\n",
            "empiezan\n",
            "las\n",
            "clases\n",
            "de\n",
            "LNRI\n",
            "de\n",
            "pr√°cticas\n",
            "y\n",
            "alguna\n",
            "cosa\n",
            "m√°s\n",
            ".\n",
            "Todo\n",
            "lo\n",
            "que\n",
            "sigue\n",
            "son\n",
            "ejemplos\n",
            "de\n",
            "acr√≥nimos\n",
            "que\n",
            "no\n",
            "se\n",
            "deber√≠an\n",
            "separar\n",
            ":\n",
            "EE.UU.\n",
            ",\n",
            "S.L.\n",
            ",\n",
            "CC.OO.\n",
            ",\n",
            "S.A.\n",
            ",\n",
            "D.\n",
            ",\n",
            "U.R.S.S.\n",
            ",\n",
            "entre\n",
            "otros\n",
            ".\n",
            "Pod√©is\n",
            "probar\n",
            "con\n",
            "otros\n",
            "ejemplos\n",
            ",\n",
            "e\n",
            "incluso\n",
            ",\n",
            "plantear\n",
            "alg√∫n\n",
            "tipo\n",
            "de\n",
            "tokens\n",
            "que\n",
            "os\n",
            "interese\n",
            ":\n",
            "disfrutaaaddd\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "üôÇ\n",
            "ü§î\n",
            "üôà\n",
            "es\n",
            "as√≠\n",
            ",\n",
            "bla\n",
            ",\n",
            "bla\n",
            ",\n",
            "bla\n",
            "üéì\n",
            "es\n",
            ",\n",
            "se\n",
            ".\n",
            "üòå\n",
            "de\n",
            "...\n",
            ";\n",
            "üíï\n",
            "üë≠\n",
            "üëô\n",
            "üòä\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}